{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T21:11:51.717207Z",
     "start_time": "2024-12-09T21:11:51.241155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class KosovoSerbiaAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize analyzer with components specific to Kosovo-Serbia context\"\"\"\n",
    "        # Initialize sentiment analyzer\n",
    "        try:\n",
    "            self.sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "        except:\n",
    "            print(\"Warning: Using basic sentiment analysis as transformer model couldn't be loaded\")\n",
    "\n",
    "        # Load spacy model\n",
    "        try:\n",
    "            self.nlp = spacy.load('en_core_web_sm')\n",
    "        except:\n",
    "            print(\"Warning: Install spaCy model with 'python -m spacy download en_core_web_sm'\")\n",
    "\n",
    "        # Define context-specific keywords\n",
    "        self.keywords = {\n",
    "            'conflict_terms': [\n",
    "                'war', 'conflict', 'tension', 'violence', 'attack', 'terrorist',\n",
    "                'terrorism', 'bomb', 'military', 'invasion', 'occupation',\n",
    "                'fight', 'battle', 'aggression', 'genocide', 'cleansing'\n",
    "            ],\n",
    "            'ethnic_identifiers': [\n",
    "                'serb', 'serbian', 'albanian', 'kosovo', 'kosova', 'albanians',\n",
    "                'serbs', 'bosniak', 'yugoslav', 'balkan'\n",
    "            ],\n",
    "            'political_terms': [\n",
    "                'nato', 'independence', 'territory', 'eu', 'recognition',\n",
    "                'parliament', 'government', 'diplomatic', 'minister', 'president'\n",
    "            ],\n",
    "            'sentiment_indicators': {\n",
    "                'positive': [\n",
    "                    'peace', 'cooperation', 'dialogue', 'progress', 'development',\n",
    "                    'support', 'recognition', 'agreement', 'friendship', 'collaboration'\n",
    "                ],\n",
    "                'negative': [\n",
    "                    'genocide', 'cleansing', 'hate', 'terrorism', 'violation',\n",
    "                    'invasion', 'occupation', 'threat', 'crisis', 'conflict'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def clean_tweet(self, tweet: str) -> str:\n",
    "        \"\"\"Clean and preprocess tweet text\"\"\"\n",
    "        if not isinstance(tweet, str):\n",
    "            return \"\"\n",
    "\n",
    "        # Convert to lowercase\n",
    "        text = tweet.lower()\n",
    "\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "        # Remove mentions\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "        # Remove hashtags but keep the text\n",
    "        text = re.sub(r'#', '', text)\n",
    "\n",
    "        # Remove emojis and special characters\n",
    "        text = re.sub(r'[^\\w\\s,.]', '', text)\n",
    "\n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        return text\n",
    "\n",
    "    def analyze_sentiment(self, tweet: str) -> Dict:\n",
    "        \"\"\"Analyze sentiment with context-specific adjustments\"\"\"\n",
    "        cleaned_text = self.clean_tweet(tweet)\n",
    "\n",
    "        try:\n",
    "            # Get base sentiment from transformer\n",
    "            base_sentiment = self.sentiment_analyzer(cleaned_text)[0]\n",
    "            sentiment_score = float(base_sentiment['score'])\n",
    "            sentiment_label = base_sentiment['label']\n",
    "        except:\n",
    "            # Fallback to basic sentiment analysis\n",
    "            positive_words = sum(1 for word in self.keywords['sentiment_indicators']['positive']\n",
    "                               if word in cleaned_text)\n",
    "            negative_words = sum(1 for word in self.keywords['sentiment_indicators']['negative']\n",
    "                               if word in cleaned_text)\n",
    "            sentiment_score = (positive_words - negative_words) / (positive_words + negative_words + 1)\n",
    "            sentiment_label = 'POSITIVE' if sentiment_score > 0 else 'NEGATIVE'\n",
    "\n",
    "        # Context adjustment\n",
    "        context_score = 0\n",
    "        for pos_term in self.keywords['sentiment_indicators']['positive']:\n",
    "            if pos_term in cleaned_text:\n",
    "                context_score += 0.1\n",
    "        for neg_term in self.keywords['sentiment_indicators']['negative']:\n",
    "            if neg_term in cleaned_text:\n",
    "                context_score -= 0.1\n",
    "\n",
    "        adjusted_score = min(1.0, max(0.0, sentiment_score + context_score))\n",
    "\n",
    "        return {\n",
    "            'sentiment': sentiment_label,\n",
    "            'base_score': sentiment_score,\n",
    "            'adjusted_score': adjusted_score,\n",
    "            'context_modification': context_score\n",
    "        }\n",
    "\n",
    "    def analyze_ethnic_references(self, tweet: str) -> Dict:\n",
    "        \"\"\"Analyze references to ethnic groups and communities\"\"\"\n",
    "        text = self.clean_tweet(tweet)\n",
    "\n",
    "        references = {\n",
    "            'serbian': 0,\n",
    "            'albanian': 0,\n",
    "            'other_ethnic': 0,\n",
    "            'balanced': False\n",
    "        }\n",
    "\n",
    "        # Count Serbian references\n",
    "        serbian_patterns = ['serb', 'serbian', 'serbia']\n",
    "        for pattern in serbian_patterns:\n",
    "            references['serbian'] += len(re.findall(r'\\b' + pattern + r'\\w*\\b', text, re.I))\n",
    "\n",
    "        # Count Albanian references\n",
    "        albanian_patterns = ['albania', 'albanian', 'kosovo']\n",
    "        for pattern in albanian_patterns:\n",
    "            references['albanian'] += len(re.findall(r'\\b' + pattern + r'\\w*\\b', text, re.I))\n",
    "\n",
    "        # Count other ethnic references\n",
    "        other_patterns = ['bosniak', 'croatian', 'macedonian']\n",
    "        for pattern in other_patterns:\n",
    "            references['other_ethnic'] += len(re.findall(r'\\b' + pattern + r'\\w*\\b', text, re.I))\n",
    "\n",
    "        # Check if reference is balanced\n",
    "        references['balanced'] = abs(references['serbian'] - references['albanian']) <= 1\n",
    "\n",
    "        return references\n",
    "\n",
    "    def analyze_conflict_indicators(self, tweet: str) -> Dict:\n",
    "        \"\"\"Analyze conflict-related content and tone\"\"\"\n",
    "        text = self.clean_tweet(tweet)\n",
    "\n",
    "        indicators = {\n",
    "            'conflict_terms': 0,\n",
    "            'peace_terms': 0,\n",
    "            'historical_reference': False,\n",
    "            'current_events': False\n",
    "        }\n",
    "\n",
    "        # Count conflict terms\n",
    "        for term in self.keywords['conflict_terms']:\n",
    "            indicators['conflict_terms'] += len(re.findall(r'\\b' + term + r'\\w*\\b', text, re.I))\n",
    "\n",
    "        # Count peace terms\n",
    "        peace_terms = ['peace', 'dialogue', 'cooperation', 'agreement', 'friendship']\n",
    "        for term in peace_terms:\n",
    "            indicators['peace_terms'] += len(re.findall(r'\\b' + term + r'\\w*\\b', text, re.I))\n",
    "\n",
    "        # Check for historical references\n",
    "        historical_patterns = ['1999', '90s', 'history', 'historical', 'ottoman', 'yugoslavia']\n",
    "        indicators['historical_reference'] = any(pattern in text for pattern in historical_patterns)\n",
    "\n",
    "        # Check for current events references\n",
    "        current_patterns = ['today', 'now', 'current', 'recent', 'latest']\n",
    "        indicators['current_events'] = any(pattern in text for pattern in current_patterns)\n",
    "\n",
    "        return indicators\n",
    "\n",
    "    def analyze_dataset(self, tweets_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Analyze entire dataset of tweets\"\"\"\n",
    "        results = []\n",
    "\n",
    "        for _, row in tweets_df.iterrows():\n",
    "            tweet = row['Tweet']\n",
    "\n",
    "            # Skip invalid tweets\n",
    "            if not isinstance(tweet, str):\n",
    "                continue\n",
    "\n",
    "            # Perform analyses\n",
    "            sentiment = self.analyze_sentiment(tweet)\n",
    "            ethnic_refs = self.analyze_ethnic_references(tweet)\n",
    "            conflict_indicators = self.analyze_conflict_indicators(tweet)\n",
    "\n",
    "            # Compile results\n",
    "            analysis = {\n",
    "                'original_tweet': tweet,\n",
    "                'cleaned_tweet': self.clean_tweet(tweet),\n",
    "                'sentiment_label': sentiment['sentiment'],\n",
    "                'sentiment_score': sentiment['adjusted_score'],\n",
    "                'serbian_references': ethnic_refs['serbian'],\n",
    "                'albanian_references': ethnic_refs['albanian'],\n",
    "                'other_ethnic_references': ethnic_refs['other_ethnic'],\n",
    "                'balanced_reference': ethnic_refs['balanced'],\n",
    "                'conflict_terms': conflict_indicators['conflict_terms'],\n",
    "                'peace_terms': conflict_indicators['peace_terms'],\n",
    "                'historical_reference': conflict_indicators['historical_reference'],\n",
    "                'current_events': conflict_indicators['current_events']\n",
    "            }\n",
    "\n",
    "            results.append(analysis)\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def cross_validate_sentiment(self, tweets_df: pd.DataFrame, n_splits=5) -> Dict:\n",
    "        \"\"\"Perform cross-validation on sentiment analysis\"\"\"\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        scores = []\n",
    "\n",
    "        # Prepare data\n",
    "        X = tweets_df['Tweet'].values\n",
    "        y = [1 if any(term in str(tweet).lower() for term in self.keywords['sentiment_indicators']['positive'])\n",
    "         else 0 for tweet in X]\n",
    "\n",
    "        for train_idx, test_idx in kf.split(X):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = np.array(y)[train_idx], np.array(y)[test_idx]\n",
    "\n",
    "            # Train on fold\n",
    "            train_predictions = [self.analyze_sentiment(str(tweet))['adjusted_score'] > 0.5\n",
    "                               for tweet in X_train]\n",
    "            test_predictions = [self.analyze_sentiment(str(tweet))['adjusted_score'] > 0.5\n",
    "                              for tweet in X_test]\n",
    "\n",
    "            # Calculate accuracy for this fold\n",
    "            accuracy = sum(p == t for p, t in zip(test_predictions, y_test)) / len(y_test)\n",
    "            scores.append(accuracy)\n",
    "\n",
    "        return {\n",
    "            'mean_accuracy': np.mean(scores),\n",
    "            'std_accuracy': np.std(scores),\n",
    "            'fold_scores': scores\n",
    "        }\n",
    "\n",
    "    def analyze_topic_distribution(self, analysis_df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Analyze distribution of main topics in tweets\"\"\"\n",
    "        topics = {\n",
    "            'political': ['government', 'president', 'parliament', 'minister', 'election', 'policy'],\n",
    "            'conflict': ['war', 'fight', 'battle', 'attack', 'military', 'weapon'],\n",
    "            'ethnic': ['serbian', 'albanian', 'ethnic', 'minority', 'community'],\n",
    "            'economic': ['business', 'economy', 'trade', 'development', 'investment'],\n",
    "            'cultural': ['tradition', 'heritage', 'religion', 'culture', 'history'],\n",
    "            'international': ['nato', 'eu', 'un', 'international', 'foreign']\n",
    "        }\n",
    "\n",
    "        topic_counts = {topic: 0 for topic in topics.keys()}\n",
    "\n",
    "        for _, row in analysis_df.iterrows():\n",
    "            text = row['cleaned_tweet'].lower()\n",
    "            for topic, keywords in topics.items():\n",
    "                if any(keyword in text for keyword in keywords):\n",
    "                    topic_counts[topic] += 1\n",
    "\n",
    "        # Calculate percentages\n",
    "        total_tweets = len(analysis_df)\n",
    "        topic_percentages = {\n",
    "            topic: (count / total_tweets) * 100\n",
    "            for topic, count in topic_counts.items()\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            'counts': topic_counts,\n",
    "            'percentages': topic_percentages\n",
    "        }\n",
    "\n",
    "    def analyze_hashtag_patterns(self, tweets_df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Analyze hashtag usage patterns\"\"\"\n",
    "        hashtag_pattern = r'#\\w+'\n",
    "        hashtags = []\n",
    "\n",
    "        for tweet in tweets_df['Tweet']:\n",
    "            if isinstance(tweet, str):\n",
    "                hashtags.extend(re.findall(hashtag_pattern, tweet))\n",
    "\n",
    "        hashtag_counts = Counter(hashtags)\n",
    "\n",
    "        return {\n",
    "            'most_common': hashtag_counts.most_common(10),\n",
    "            'total_hashtags': len(hashtags),\n",
    "            'unique_hashtags': len(hashtag_counts),\n",
    "            'hashtag_frequency': dict(hashtag_counts)\n",
    "        }\n",
    "\n",
    "    def analyze_user_interactions(self, tweets_df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Analyze user interaction patterns\"\"\"\n",
    "        mention_pattern = r'@\\w+'\n",
    "        mentions = []\n",
    "\n",
    "        for tweet in tweets_df['Tweet']:\n",
    "            if isinstance(tweet, str):\n",
    "                mentions.extend(re.findall(mention_pattern, tweet))\n",
    "\n",
    "        mention_counts = Counter(mentions)\n",
    "\n",
    "        return {\n",
    "            'most_mentioned': mention_counts.most_common(10),\n",
    "            'total_mentions': len(mentions),\n",
    "            'unique_mentions': len(mention_counts),\n",
    "            'mention_frequency': dict(mention_counts)\n",
    "        }\n",
    "\n",
    "    def analyze_temporal_patterns(self, analysis_df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Analyze patterns over time in the dataset\"\"\"\n",
    "        # Create rolling averages for different metrics\n",
    "        window_size = 10\n",
    "\n",
    "        temporal_patterns = {\n",
    "            'sentiment_trend': analysis_df['sentiment_score'].rolling(window=window_size).mean(),\n",
    "            'conflict_terms_trend': analysis_df['conflict_terms'].rolling(window=window_size).mean(),\n",
    "            'peace_terms_trend': analysis_df['peace_terms'].rolling(window=window_size).mean(),\n",
    "        }\n",
    "\n",
    "        # Calculate correlation between different metrics\n",
    "        correlations = {\n",
    "            'sentiment_conflict': analysis_df['sentiment_score'].corr(analysis_df['conflict_terms']),\n",
    "            'sentiment_peace': analysis_df['sentiment_score'].corr(analysis_df['peace_terms']),\n",
    "            'conflict_peace': analysis_df['conflict_terms'].corr(analysis_df['peace_terms'])\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            'trends': temporal_patterns,\n",
    "            'correlations': correlations\n",
    "        }\n",
    "\n",
    "    def analyze_language_complexity(self, analysis_df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Analyze complexity and sophistication of language used\"\"\"\n",
    "        complexity_metrics = {\n",
    "            'avg_tweet_length': [],\n",
    "            'word_count': [],\n",
    "            'unique_words': []\n",
    "        }\n",
    "\n",
    "        for tweet in analysis_df['cleaned_tweet']:\n",
    "            words = tweet.split()\n",
    "            complexity_metrics['avg_tweet_length'].append(len(tweet))\n",
    "            complexity_metrics['word_count'].append(len(words))\n",
    "            complexity_metrics['unique_words'].append(len(set(words)))\n",
    "\n",
    "        return {\n",
    "            'averages': {\n",
    "                'avg_length': np.mean(complexity_metrics['avg_tweet_length']),\n",
    "                'avg_words': np.mean(complexity_metrics['word_count']),\n",
    "                'avg_unique': np.mean(complexity_metrics['unique_words'])\n",
    "            },\n",
    "            'distributions': complexity_metrics\n",
    "        }\n",
    "\n",
    "    def visualize_analysis(self, analysis_df: pd.DataFrame, save_path: str = None):\n",
    "        \"\"\"Generate comprehensive visualizations of the analysis\"\"\"\n",
    "        plt.style.use('seaborn')\n",
    "\n",
    "        # Create figure with subplots\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "\n",
    " # 1. Sentiment Distribution\n",
    "        plt.subplot(2, 2, 1)\n",
    "        sns.histplot(data=analysis_df, x='sentiment_score', bins=20)\n",
    "        sns.set_style(\"whitegrid\")  # or any other style\n",
    "        plt.title('Distribution of Sentiment Scores')\n",
    "        plt.xlabel('Sentiment Score')\n",
    "        plt.ylabel('Count')\n",
    "\n",
    "        # 2. Ethnic References\n",
    "        plt.subplot(2, 2, 2)\n",
    "        ethnic_data = {\n",
    "            'Serbian': analysis_df['serbian_references'].sum(),\n",
    "            'Albanian': analysis_df['albanian_references'].sum(),\n",
    "            'Other': analysis_df['other_ethnic_references'].sum()\n",
    "        }\n",
    "        plt.pie(ethnic_data.values(), labels=ethnic_data.keys(), autopct='%1.1f%%')\n",
    "        plt.title('Distribution of Ethnic References')\n",
    "\n",
    "        # 3. Conflict vs Peace Terms\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.scatter(analysis_df['conflict_terms'], analysis_df['peace_terms'],\n",
    "                   alpha=0.5, c=analysis_df['sentiment_score'], cmap='RdYlBu')\n",
    "        plt.colorbar(label='Sentiment Score')\n",
    "        plt.title('Conflict vs Peace Terms Usage')\n",
    "        plt.xlabel('Number of Conflict Terms')\n",
    "        plt.ylabel('Number of Peace Terms')\n",
    "\n",
    "        # 4. Time References\n",
    "        plt.subplot(2, 2, 4)\n",
    "        time_refs = pd.Series({\n",
    "            'Historical': sum(analysis_df['historical_reference']),\n",
    "            'Current': sum(analysis_df['current_events']),\n",
    "            'Unspecified': len(analysis_df) - sum(analysis_df['historical_reference'])\n",
    "                          - sum(analysis_df['current_events'])\n",
    "        })\n",
    "        time_refs.plot(kind='bar')\n",
    "        plt.title('Distribution of Time References')\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(f\"{save_path}/analysis_overview.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def generate_wordclouds(self, analysis_df: pd.DataFrame, save_path: str = None):\n",
    "        \"\"\"Generate word clouds for different sentiment categories\"\"\"\n",
    "        # Create figure with subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "        # Positive tweets wordcloud\n",
    "        positive_tweets = ' '.join(analysis_df[analysis_df['sentiment_score'] > 0.6]['cleaned_tweet'])\n",
    "        wordcloud_pos = WordCloud(\n",
    "            width=800, height=400,\n",
    "            background_color='white',\n",
    "            colormap='YlGn',\n",
    "            max_words=100\n",
    "        ).generate(positive_tweets)\n",
    "\n",
    "        ax1.imshow(wordcloud_pos)\n",
    "        ax1.axis('off')\n",
    "        ax1.set_title('Positive Content Word Cloud')\n",
    "\n",
    "        # Negative tweets wordcloud\n",
    "        negative_tweets = ' '.join(analysis_df[analysis_df['sentiment_score'] < 0.4]['cleaned_tweet'])\n",
    "        wordcloud_neg = WordCloud(\n",
    "            width=800, height=400,\n",
    "            background_color='white',\n",
    "            colormap='RdGy',\n",
    "            max_words=100\n",
    "        ).generate(negative_tweets)\n",
    "\n",
    "        ax2.imshow(wordcloud_neg)\n",
    "        ax2.axis('off')\n",
    "        ax2.set_title('Negative Content Word Cloud')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(f\"{save_path}/wordclouds.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def export_analysis(self, analysis_df: pd.DataFrame, output_dir: str = 'output'):\n",
    "        \"\"\"Export analysis results in multiple formats\"\"\"\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Save main analysis results\n",
    "        analysis_df.to_csv(f\"{output_dir}/analysis_results.csv\", index=False)\n",
    "        analysis_df.to_excel(f\"{output_dir}/analysis_results.xlsx\", index=False)\n",
    "\n",
    "        # Generate and save summary statistics\n",
    "        summary = {\n",
    "            'basic_stats': analysis_df.describe().to_dict(),\n",
    "            'topic_distribution': self.analyze_topic_distribution(analysis_df),\n",
    "            'language_complexity': self.analyze_language_complexity(analysis_df),\n",
    "            'temporal_patterns': {\n",
    "                'correlations': self.analyze_temporal_patterns(analysis_df)['correlations']\n",
    "            }\n",
    "        }\n",
    "\n",
    "        with open(f\"{output_dir}/analysis_summary.json\", 'w') as f:\n",
    "            json.dump(summary, f, indent=4)\n",
    "\n",
    "        # Save the complete analysis object for later use\n",
    "        with open(f\"{output_dir}/analysis_object.pkl\", 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'analysis_df': analysis_df,\n",
    "                'summary': summary\n",
    "            }, f)\n",
    "\n",
    "        print(f\"Analysis results exported to {output_dir}/\")\n",
    "\n",
    "    def create_interactive_dashboard(self, analysis_df: pd.DataFrame):\n",
    "        \"\"\"Create an interactive dashboard using plotly\"\"\"\n",
    "        # Create subplot figure\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=(\n",
    "                'Sentiment Distribution',\n",
    "                'Ethnic References',\n",
    "                'Conflict vs Peace Terms',\n",
    "                'Time References'\n",
    "            ),\n",
    "            specs=[\n",
    "                [{\"type\": \"histogram\"}, {\"type\": \"pie\"}],\n",
    "                [{\"type\": \"scatter\"}, {\"type\": \"bar\"}]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 1. Sentiment Distribution\n",
    "        fig.add_trace(\n",
    "            go.Histogram(\n",
    "                x=analysis_df['sentiment_score'],\n",
    "                nbinsx=20,\n",
    "                name='Sentiment'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "        # 2. Ethnic References\n",
    "        ethnic_data = {\n",
    "            'Serbian': analysis_df['serbian_references'].sum(),\n",
    "            'Albanian': analysis_df['albanian_references'].sum(),\n",
    "            'Other': analysis_df['other_ethnic_references'].sum()\n",
    "        }\n",
    "        fig.add_trace(\n",
    "            go.Pie(\n",
    "                labels=list(ethnic_data.keys()),\n",
    "                values=list(ethnic_data.values()),\n",
    "                name='Ethnic References'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "        # 3. Conflict vs Peace Terms\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=analysis_df['conflict_terms'],\n",
    "                y=analysis_df['peace_terms'],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    color=analysis_df['sentiment_score'],\n",
    "                    colorscale='RdYlBu',\n",
    "                    showscale=True\n",
    "                ),\n",
    "                name='Terms Usage'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "        # 4. Time References\n",
    "        time_refs = pd.Series({\n",
    "            'Historical': sum(analysis_df['historical_reference']),\n",
    "            'Current': sum(analysis_df['current_events']),\n",
    "            'Unspecified': len(analysis_df) - sum(analysis_df['historical_reference'])\n",
    "                          - sum(analysis_df['current_events'])\n",
    "        })\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=time_refs.index,\n",
    "                y=time_refs.values,\n",
    "                name='Time References'\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "\n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            showlegend=False,\n",
    "            title_text=\"Kosovo-Serbia Tweet Analysis Dashboard\",\n",
    "            title_x=0.5\n",
    "        )\n",
    "\n",
    "        return fig\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "\n",
    "    try:\n",
    "        # Load the dataset\n",
    "        print(\"Loading dataset...\")\n",
    "        # tweets_df = pd.read_csv(\"config/tweets.csv\")\n",
    "\n",
    "        tweets_df = pd.read_csv(\"config/tweets.csv\", encoding='latin-1')  # or 'cp1252'\n",
    "\n",
    "        # Initialize analyzer\n",
    "        print(\"Initializing analyzer...\")\n",
    "        analyzer = KosovoSerbiaAnalyzer()\n",
    "\n",
    "        # Perform cross-validation\n",
    "        print(\"Performing cross-validation...\")\n",
    "        cv_results = analyzer.cross_validate_sentiment(tweets_df)\n",
    "        print(f\"\\nCross-validation results:\")\n",
    "        print(f\"Mean accuracy: {cv_results['mean_accuracy']:.3f} (Â±{cv_results['std_accuracy']:.3f})\")\n",
    "\n",
    "        # Analyze tweets\n",
    "        print(\"\\nAnalyzing tweets...\")\n",
    "        analysis_results = analyzer.analyze_dataset(tweets_df)\n",
    "\n",
    "        # Generate visualizations\n",
    "        print(\"\\nGenerating visualizations...\")\n",
    "        analyzer.visualize_analysis(analysis_results, save_path='visualizations')\n",
    "        analyzer.generate_wordclouds(analysis_results, save_path='visualizations')\n",
    "\n",
    "        # Create and save interactive dashboard\n",
    "        print(\"\\nCreating interactive dashboard...\")\n",
    "        dashboard = analyzer.create_interactive_dashboard(analysis_results)\n",
    "        dashboard.write_html(\"visualizations/dashboard.html\")\n",
    "\n",
    "        # Export results\n",
    "        print(\"\\nExporting analysis results...\")\n",
    "        analyzer.export_analysis(analysis_results)\n",
    "\n",
    "        print(\"\\nAnalysis complete! Check the output directory for results.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError occurred: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "4b269e425edbfd2f",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mseaborn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msns\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pipeline\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b740ce652abbdb71"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
